{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8840f571",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.optimize import curve_fit\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#------------------------------------------\n",
    "# Create a list of colors\n",
    "colors_rarefaction=sns.color_palette(\"colorblind\", n_colors=len(list(snakemake.params.samples))+1)\n",
    "# Create a LinearSegmentedColormap object\n",
    "cmap1=LinearSegmentedColormap.from_list(\"my_colormap\", sns.color_palette(\"colorblind\", n_colors=5))\n",
    "\n",
    "\n",
    "sns.set_style(\"ticks\",{'axes.grid' : True})\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.5\n",
    "plt.rcParams[\"xtick.major.width\"] = 1.5\n",
    "plt.rcParams[\"ytick.major.width\"] = 1.5\n",
    "plt.rcParams[\"xtick.major.size\"] = 8\n",
    "plt.rcParams[\"ytick.major.size\"] = 8\n",
    "plt.rcParams[\"axes.titlepad\"] = 20\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams[\"axes.titlesize\"] = 30\n",
    "plt.rcParams['axes.labelsize'] = 23.5\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Liberation Sans']\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams[\"savefig.dpi\"]=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dir=snakemake.params.clean_dir\n",
    "raw_dir=snakemake.params.raw_dir\n",
    "qc_dir=snakemake.params.qc_dir\n",
    "\n",
    "SAMPLE_PLOT_LIM_LEGEND=100\n",
    "SAMPLE_PLOT_LIM=200\n",
    "\n",
    "SAMPLES=list(snakemake.params.samples)\n",
    "forward_tag=\"_\" + str(snakemake.params.forward_tag)\n",
    "reverse_tag=\"_\" +str( snakemake.params.reverse_tag)\n",
    "\n",
    "input_histograms=list(snakemake.input.histograms)\n",
    "input_histograms_pre=list(snakemake.input.histogram_kmer_pre)\n",
    "input_histograms_post=list(snakemake.input.histogram_kmer_post)\n",
    "# input_peak_kmer=list(snakemake.input.peak_kmer)\n",
    "\n",
    "input_histograms.sort()\n",
    "input_histograms_pre.sort()\n",
    "input_histograms_post.sort()\n",
    "\n",
    "\n",
    "if len(input_histograms)>SAMPLE_PLOT_LIM:\n",
    "   input_histograms=input_histograms[:SAMPLE_PLOT_LIM]\n",
    "\n",
    "if len(input_histograms_pre)>SAMPLE_PLOT_LIM:\n",
    "   input_histograms_pre=input_histograms_pre[:SAMPLE_PLOT_LIM]\n",
    "\n",
    "if len(input_histograms_post)>SAMPLE_PLOT_LIM:\n",
    "   input_histograms_post=input_histograms_post[:SAMPLE_PLOT_LIM]\n",
    "\n",
    "illumina_preqc=snakemake.input.preqc_txt\n",
    "illumina_postqc=snakemake.input.postqc_txt\n",
    "#------------------------------------------\n",
    "output_kmer_png=snakemake.output.kmer_png\n",
    "output_kmer_svg=snakemake.output.kmer_svg\n",
    "output_kmer_png_fitted=snakemake.output.kmer_fit_png\n",
    "output_kmer_svg_fitted=snakemake.output.kmer_fit_svg\n",
    "output_kmer_fit_html=snakemake.output.kmer_fit_html\n",
    "output_kmer_dist_pre_png=snakemake.output.kmer_dist_pre_png\n",
    "output_kmer_dist_pre_svg=snakemake.output.kmer_dist_pre_svg\n",
    "output_kmer_dist_post_png=snakemake.output.kmer_dist_post_png\n",
    "output_kmer_dist_post_svg=snakemake.output.kmer_dist_post_svg\n",
    "\n",
    "output_qc_summary_html=snakemake.output.qc_summary_html\n",
    "output_percentage_kept_reads_png=snakemake.output.percentage_kept_reads_png\n",
    "output_percentage_kept_reads_svg=snakemake.output.percentage_kept_reads_svg\n",
    "output_percentage_kept_Mbp_png=snakemake.output.percentage_kept_Mbp_png\n",
    "output_percentage_kept_Mbp_svg=snakemake.output.percentage_kept_Mbp_svg\n",
    "output_step_qc_reads_html=snakemake.output.step_qc_reads_html\n",
    "output_steps_qc_reads_png=snakemake.output.steps_qc_reads_png\n",
    "output_steps_qc_reads_svg=snakemake.output.steps_qc_reads_svg\n",
    "output_steps_qc_percentage_png=snakemake.output.steps_qc_percentage_png\n",
    "output_steps_qc_percentage_svg=snakemake.output.steps_qc_percentage_svg\n",
    "output_df_counts_paired=snakemake.output.df_counts_paired\n",
    "output_supperdedupper_html=snakemake.output.supperdedupper_html\n",
    "output_supperdedupper_png=snakemake.output.supperdedupper_png\n",
    "output_supperdedupper_svg=snakemake.output.supperdedupper_svg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a4b5394",
   "metadata": {},
   "source": [
    "## KMER rarefraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure size and font scale for seaborn\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Sort input_histograms and SAMPLES\n",
    "input_histograms.sort()\n",
    "SAMPLES.sort()\n",
    "\n",
    "# Initialize variables\n",
    "n = 1\n",
    "read_max = 0\n",
    "\n",
    "# Create a single plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Loop through each histogram in input_histograms\n",
    "for h in input_histograms:\n",
    "    # Read the histogram file as a pandas dataframe\n",
    "    df = pd.read_csv(h, sep=\"\\t\", usecols=[\"#count\", \"first\"])\n",
    "    df.columns=[\"counts\", \"percent\"]\n",
    "    \n",
    "    # Convert percent to percentage of missing kmers and counts to millions of reads\n",
    "    df[\"percent\"] = 100 - df[\"percent\"]\n",
    "    df[\"counts\"] = df[\"counts\"] / 1000000\n",
    "\n",
    "    # Plot the data as a line plot\n",
    "    sns.lineplot(x=\"counts\", y=\"percent\", data=df, err_style='band', color=colors_rarefaction[n], \n",
    "                 label=h.split(\"/\")[-1].split(\"_kmer\")[0], linewidth=0.5, ax=ax)\n",
    "\n",
    "    # Add a vertical line at the x-value corresponding to the maximum count\n",
    "    max_count = df[\"counts\"].max()\n",
    "    plt.axvline(max_count, 0, 1, linestyle=\"--\", color=colors_rarefaction[n], alpha=0.3)\n",
    "    read_max = max(read_max, max_count)\n",
    "\n",
    "    # Increment n\n",
    "    print(f\"{100*n/len(input_histograms):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "    n += 1\n",
    "\n",
    "# Set y- and x-axis limits, labels, and legend outside the loop\n",
    "ax.set(ylim=(0, 110))\n",
    "ax.set(xlim=(0, read_max * 1.2))\n",
    "ax.set_xlabel(\"Read count (Million reads)\")\n",
    "ax.set_ylabel(\"Known k-mers (%)\")\n",
    "if len(SAMPLES) <= SAMPLE_PLOT_LIM_LEGEND:\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=int(np.ceil(len(SAMPLES) / 25)))\n",
    "else:\n",
    "    ax.get_legend().set_visible(False)\n",
    "    \n",
    "# Save the figure as png and svg files\n",
    "fig.savefig(output_kmer_png, format=\"png\", bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(output_kmer_svg, format=\"svg\", bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47494c0e",
   "metadata": {},
   "source": [
    "# KMER rarefraction log fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size and font scale for seaborn\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Define the number of million reads to trim\n",
    "cut_million_reads = 5\n",
    "trim_reads = int(cut_million_reads * 40)\n",
    "max_read_fitting = 80\n",
    "\n",
    "# Initialize variables\n",
    "read_max = 0\n",
    "n = 1\n",
    "samples = []\n",
    "a_list = []\n",
    "b_list = []\n",
    "max_reads = []\n",
    "slope_list = []\n",
    "\n",
    "# Define the logarithmic function used for fitting\n",
    "def logFunc(x, a, b):\n",
    "    return a + b * np.log(x)\n",
    "\n",
    "# Create a single plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Loop through input histograms\n",
    "for h in input_histograms:\n",
    "    # Load the data from the current input histogram\n",
    "    df = pd.read_csv(h, sep=\"\\t\", usecols=[\"#count\", \"first\"])\n",
    "    df.columns=[\"counts\", \"percent\"]\n",
    "    \n",
    "    # Calculate the percentage of known k-mers and convert the counts to millions of reads\n",
    "    df[\"percent\"] = 100 - df[\"percent\"]\n",
    "    df[\"counts\"] = df[\"counts\"] / 1000000\n",
    "\n",
    "    # Plot the original data\n",
    "    sns.lineplot(x=\"counts\", y=\"percent\", data=df, err_style='band', color=colors_rarefaction[n], \n",
    "                 label=h.split(\"/\")[-1].split(\"_kmer\")[0], linewidth=0.5, alpha=0.8, ax=ax)\n",
    "\n",
    "    # Keep track of the maximum read count\n",
    "    read_max = max(read_max, df[\"counts\"].max())\n",
    "\n",
    "    # If there are enough data points, fit a logarithmic function to the tail of the data\n",
    "    if len(df[\"counts\"]) > trim_reads + 1:\n",
    "        # Subsample pandas dataframe every half million reads\n",
    "        subsample_rate_mreads = 0.1\n",
    "        subsample_size = int(subsample_rate_mreads * 40)\n",
    "        df_subsampled = df[trim_reads:].iloc[::subsample_size, :]\n",
    "        if len(df_subsampled) > 10:\n",
    "            # Fit the function to the tail of the data\n",
    "            popt, _ = curve_fit(logFunc, df_subsampled[\"counts\"], df_subsampled[\"percent\"])\n",
    "\n",
    "            # Plot the fitted function\n",
    "            sns.lineplot(x=np.arange(cut_million_reads, max_read_fitting, 0.25), \n",
    "                         y=logFunc(np.arange(cut_million_reads, max_read_fitting, 0.25), *popt), \n",
    "                         color=colors_rarefaction[n], err_style='band', linewidth=2, alpha=0.2, \n",
    "                         label='_nolegend_', ax=ax)\n",
    "\n",
    "            # Store the fitting parameters and other statistics\n",
    "            samples.append(h.split(\"/\")[-1].split(\"_kmer\")[0])\n",
    "            a_list.append(popt[0])\n",
    "            b_list.append(popt[1])\n",
    "            max_reads.append(df[\"counts\"].max())\n",
    "            reads_to_1 = (popt[0] / popt[1])\n",
    "            slope_list.append(popt[1] / df[\"counts\"].max())\n",
    "\n",
    "    # Add a vertical line to indicate the maximum read count\n",
    "    plt.axvline(df[\"counts\"].max(), 0, 1, linestyle=\"--\", color=colors_rarefaction[n], alpha=0.4)\n",
    "\n",
    "    # Increment the sample counter\n",
    "    print(f\"{100*n/len(input_histograms):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "    n += 1\n",
    "\n",
    "# Set y- and x-axis limits, labels, and legend outside the loop\n",
    "ax.set(ylim=(0, 110))\n",
    "ax.set(xlim=(0, read_max * 1.5))\n",
    "ax.set_xlabel(\"Read count (Million reads)\")\n",
    "ax.set_ylabel(\"Known k-mers (%)\")\n",
    "if len(SAMPLES) <= SAMPLE_PLOT_LIM_LEGEND:\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=int(np.ceil(len(SAMPLES) / 25)))\n",
    "else:\n",
    "    ax.get_legend().set_visible(False)\n",
    "    \n",
    "# Save the figure as png and svg files\n",
    "fig.savefig(output_kmer_png_fitted, format=\"png\", bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(output_kmer_svg_fitted, format=\"svg\", bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b258ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new empty pandas DataFrame\n",
    "stats_df=pd.DataFrame()\n",
    "\n",
    "# add columns to the DataFrame using lists\n",
    "stats_df[\"Sample\"]=samples\n",
    "stats_df[\"a\"]=a_list\n",
    "stats_df[\"b\"]=b_list\n",
    "stats_df[\"slope\"]=slope_list\n",
    "stats_df[\"M_reads\"]=max_reads\n",
    "stats_df[\"b_-_Mreads\"]=stats_df[\"b\"]-stats_df[\"M_reads\"]\n",
    "\n",
    "# sort the DataFrame by \"Sample\" column, apply background color gradient to it, and render it as an HTML table\n",
    "stats_df_out=stats_df.sort_values(by=\"Sample\").style.background_gradient(cmap=\"RdYlGn\").render()\n",
    "\n",
    "# write the HTML table to a file\n",
    "with open(output_kmer_fit_html,\"w\") as fp:\n",
    "    fp.write(stats_df_out)\n",
    "\n",
    "# sort the DataFrame by \"Sample\" column and apply background color gradient to it\n",
    "stats_df.sort_values(by=\"Sample\").style.background_gradient(cmap=\"RdYlGn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b14c664d",
   "metadata": {},
   "source": [
    "# Kmer count histogram (kmer size=31)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a20acc2e",
   "metadata": {},
   "source": [
    "## Pre normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure size and font scale for seaborn\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# Sort input_histograms and print the sorted list\n",
    "input_histograms_pre.sort()\n",
    "\n",
    "# Sort SAMPLES\n",
    "SAMPLES.sort()\n",
    "\n",
    "# Initialize n and read_max as 0\n",
    "n=1\n",
    "\n",
    "# Loop through each histogram in input_histograms\n",
    "for h in input_histograms_pre:\n",
    "    # Read the histogram file as a pandas dataframe\n",
    "    df=pd.read_csv(h, sep=\"\\t\")\n",
    "    \n",
    "    # Rename columns \n",
    "    df.columns=[\"Depth\", \"Raw count\", \"Unique kmers\"]\n",
    "\n",
    "     # Plot the data as a line plot\n",
    "    ax = sns.lineplot(x=\"Depth\", y=\"Raw count\", data=df[:],err_style='band',color=colors_rarefaction[n], label=h.split(\"/\")[-1].split(\"_kmer\")[0], linewidth=1)\n",
    "\n",
    "    # Set y- and x-axis scale\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Increment the sample counter\n",
    "    print(f\"{100*n/len(input_histograms):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "    n = n+1\n",
    "\n",
    "# Set x- and y-axis labels and legend\n",
    "ax.set_xlabel(\"Depth (31mer)\")\n",
    "ax.set_ylabel(\"Raw Count\")\n",
    "if len(SAMPLES) <= SAMPLE_PLOT_LIM_LEGEND:\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=int(np.ceil(len(SAMPLES) / 25)))\n",
    "else:\n",
    "    ax.get_legend().set_visible(False)\n",
    "# Save the figure as png and svg files\n",
    "ax.figure.savefig(output_kmer_dist_pre_png, format=\"png\", bbox_inches = \"tight\",transparent=True)\n",
    "ax.figure.savefig(output_kmer_dist_pre_svg, format=\"svg\", bbox_inches = \"tight\",transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcc5251e",
   "metadata": {},
   "source": [
    "## Post normalization (target=100x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure size and font scale for seaborn\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# Sort input_histograms and print the sorted list\n",
    "input_histograms_post.sort()\n",
    "\n",
    "# Sort SAMPLES\n",
    "SAMPLES.sort()\n",
    "\n",
    "# Initialize n and read_max as 0\n",
    "n=1\n",
    "\n",
    "# Loop through each histogram in input_histograms\n",
    "for h in input_histograms_post:\n",
    "    # Read the histogram file as a pandas dataframe\n",
    "    df=pd.read_csv(h, sep=\"\\t\")\n",
    "    \n",
    "    # Rename columns \n",
    "    df.columns=[\"Depth\", \"Raw count\", \"Unique kmers\"]\n",
    "\n",
    "     # Plot the data as a line plot\n",
    "    ax = sns.lineplot(x=\"Depth\", y=\"Raw count\", data=df[:],err_style='band',color=colors_rarefaction[n], label=h.split(\"/\")[-1].split(\"_kmer\")[0], linewidth=1)\n",
    "\n",
    "    # Set y- and x-axis scale\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Increment the sample counter\n",
    "    n = n+1\n",
    "\n",
    "# Set x- and y-axis labels and legend\n",
    "ax.set_xlabel(\"Depth (31mer)\")\n",
    "ax.set_ylabel(\"Raw Count\")\n",
    "if len(SAMPLES) <= SAMPLE_PLOT_LIM_LEGEND:\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=int(np.ceil(len(SAMPLES) / 25)))\n",
    "else:\n",
    "    ax.get_legend().set_visible(False)\n",
    "    \n",
    "# Save the figure as png and svg files\n",
    "ax.figure.savefig(output_kmer_dist_post_png, format=\"png\", bbox_inches = \"tight\",transparent=True)\n",
    "ax.figure.savefig(output_kmer_dist_post_svg, format=\"svg\", bbox_inches = \"tight\",transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6395eef3",
   "metadata": {},
   "source": [
    "## PreQC and PostQC FASTQC statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9afa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre- and post-illumina sequencing quality control data into pandas dataframes\n",
    "preqc = pd.read_csv(illumina_preqc, sep=\"\\t\")\n",
    "postqc = pd.read_csv(illumina_postqc, sep=\"\\t\")\n",
    "\n",
    "# create a new dataframe with columns for Filename, average sequence length, and total sequences from the preqc dataframe\n",
    "read_stats_df1 = preqc[[\"Filename\",\"avg_sequence_length\", \"Total Sequences\"]]\n",
    "\n",
    "# calculate the total number of megabases for each file and add this as a new column to read_stats_df1\n",
    "read_stats_df1[\"Mbp\"] = read_stats_df1[\"avg_sequence_length\"] * read_stats_df1[\"Total Sequences\"] / 1000000\n",
    "\n",
    "# remove the \".fastq\" extension from the Filename column\n",
    "read_stats_df1[\"Filename\"] = read_stats_df1[\"Filename\"].str.split(\".fastq\").str[0]\n",
    "\n",
    "# rename the columns of read_stats_df1\n",
    "read_stats_df1.columns = [\"file_pre\",\"length_pre\", \"number_pre\", \"Mbp_pre\"]\n",
    "\n",
    "# set the Filename column as the index of read_stats_df1\n",
    "read_stats_df1 = read_stats_df1.set_index('file_pre')\n",
    "\n",
    "# remove any whitespace from the index values of read_stats_df1\n",
    "read_stats_df1.index = read_stats_df1.index.map(\"\".join)\n",
    "\n",
    "# create a new dataframe with columns for Filename, average sequence length, and total sequences from the postqc dataframe\n",
    "read_stats_df2 = postqc[[\"Filename\",\"avg_sequence_length\", \"Total Sequences\"]]\n",
    "\n",
    "# calculate the total number of megabases for each file and add this as a new column to read_stats_df2\n",
    "read_stats_df2[\"Mbp\"] = read_stats_df2[\"avg_sequence_length\"] * read_stats_df2[\"Total Sequences\"] / 1000000\n",
    "\n",
    "# replace certain portions of the Filename column with standard suffixes (e.g. _forward -> _R1)\n",
    "read_stats_df2[\"Filename\"] = read_stats_df2[\"Filename\"].str.replace(\"_forward\", forward_tag)\n",
    "read_stats_df2[\"Filename\"] = read_stats_df2[\"Filename\"].str.replace(\"_reverse\", reverse_tag)\n",
    "read_stats_df2[\"Filename\"] = read_stats_df2[\"Filename\"].str.replace(\"_unpaired\", \"_U\")\n",
    "read_stats_df2[\"Filename\"] = read_stats_df2[\"Filename\"].str.split(\"_paired\").str[0]\n",
    "read_stats_df2[\"Filename\"] = read_stats_df2[\"Filename\"].str.split(\"_clean\").str[0]\n",
    "\n",
    "# create a new column in read_stats_df2 for the sample name, based on the first part of the Filename (before the final underscore)\n",
    "read_stats_df2[\"Sample\"] = read_stats_df2[\"Filename\"].str.rsplit(\"_\",1).str[0]\n",
    "\n",
    "# set the Filename column as the index of read_stats_df2\n",
    "read_stats_df2 = read_stats_df2.set_index('Filename')\n",
    "\n",
    "# rename the columns of read_stats_df2\n",
    "read_stats_df2.columns=[\"length_post\", \"number_post\", \"Mbp_post\", \"sample\"]\n",
    "\n",
    "# merge read_stats_df1 and read_stats_df2 on their indices (i.e. Filename), keeping all rows from both dataframes\n",
    "read_stats_df3 = read_stats_df1.merge(read_stats_df2, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "# replace any NaN (missing) values in read_stats_df3 with 0\n",
    "read_stats_df3 = read_stats_df3.fillna(0)\n",
    "\n",
    "# return the final merged dataframe\n",
    "read_stats_df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f88d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre quality control fastQC statistics\n",
    "preqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post quality control fastQC statistics\n",
    "postqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group read statistics by sample\n",
    "read_stats_df = read_stats_df3.groupby([\"sample\"]).sum()\n",
    "\n",
    "# Calculate percentage of reads and Mbp kept\n",
    "read_stats_df[\"%number_kept\"] = (read_stats_df[\"number_post\"] / read_stats_df[\"number_pre\"] * 100).round(2)\n",
    "read_stats_df[\"%Mbp_kept\"] = (read_stats_df[\"Mbp_post\"] / read_stats_df[\"Mbp_pre\"] * 100).round(2)\n",
    "\n",
    "# Calculate percentage of reads and Mbp removed\n",
    "read_stats_df[\"%Mbp_removed\"] = 100 - read_stats_df[\"%Mbp_kept\"]\n",
    "read_stats_df[\"%number_removed\"] = 100 - read_stats_df[\"%number_kept\"]\n",
    "\n",
    "# sort the DataFrame by \"Sample\" column, apply background color gradient to it, and render it as an HTML table\n",
    "read_stats_out=read_stats_df.drop([\"length_pre\", \"length_post\"], axis=1).reset_index().style.background_gradient(cmap=\"RdYlGn\").render()\n",
    "\n",
    "# write the HTML table to a file\n",
    "with open(output_qc_summary_html,\"w\") as fp:\n",
    "    fp.write(read_stats_out)\n",
    "    \n",
    "# Drop unnecessary columns and apply a gradient color scheme to the resulting DataFrame\n",
    "read_stats_df.drop([\"length_pre\", \"length_post\"], axis=1).reset_index().style.background_gradient(cmap=\"RdYlGn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60a7fd73",
   "metadata": {},
   "source": [
    "### Percentage of kept/removed reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d15f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size and font scale for seaborn\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Define maximum number of samples per subplot\n",
    "max_samples_per_subplot = 200\n",
    "\n",
    "# Initialize n\n",
    "n=1\n",
    "\n",
    "# Calculate the number of rows of subplots needed\n",
    "num_subplots = int(np.ceil(len(SAMPLES) / max_samples_per_subplot))\n",
    "\n",
    "# Calculate the approximate number of samples per subplot\n",
    "samples_per_subplot = int(np.ceil(len(SAMPLES) / num_subplots))\n",
    "\n",
    "# Create subplots with fixed width for each subplot\n",
    "fig, axs = plt.subplots(num_subplots, 1, figsize=(samples_per_subplot * 0.4, 12 * num_subplots), squeeze=False)\n",
    "\n",
    "# Plot each subset of samples in separate subplots\n",
    "for i in range(num_subplots):\n",
    "    start_idx = i * samples_per_subplot\n",
    "    end_idx = min((i + 1) * samples_per_subplot, len(SAMPLES))\n",
    "    subset_df = read_stats_df.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Create a bar plot of the percentage of reads kept and removed for each sample\n",
    "    ax = subset_df[[\"%number_kept\", \"%number_removed\"]].plot(\n",
    "        kind='bar', stacked=True, colormap=cmap1, width=0.8, edgecolor='none', ax=axs[i, 0]\n",
    "    )\n",
    "\n",
    "    # Add axis labels and a title to the plot\n",
    "    ax.set_xlabel('Sample')\n",
    "    ax.set_ylabel('Percentage of reads')\n",
    "    ax.set_title(f'Percentage of kept reads (Samples {start_idx + 1} to {end_idx})')\n",
    "\n",
    "    # Set the limits of the y-axis\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "    # Add a legend to the plot\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(reversed(handles), reversed(labels), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    print(f\"{100*n/(num_subplots):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "\n",
    "    n=n+1\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as PNG and SVG files\n",
    "fig.savefig(output_percentage_kept_reads_png, format=\"png\", bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(output_percentage_kept_reads_svg, format=\"svg\", bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea9c3d6f",
   "metadata": {},
   "source": [
    "### Percentage of kept/removed Mbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize n\n",
    "n=1\n",
    "\n",
    "# Create subplots with fixed width for each subplot\n",
    "fig, axs = plt.subplots(num_subplots, 1, figsize=(samples_per_subplot * 0.4, 12 * num_subplots), squeeze=False)\n",
    "\n",
    "# Plot each subset of samples in separate subplots\n",
    "for i in range(num_subplots):\n",
    "    start_idx = i * samples_per_subplot\n",
    "    end_idx = min((i + 1) * samples_per_subplot, len(SAMPLES))\n",
    "    subset_df = read_stats_df.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Create a bar plot of the percentage of Mbp kept and removed for each sample\n",
    "    ax = subset_df[[\"%Mbp_kept\", \"%Mbp_removed\"]].plot(\n",
    "        kind='bar', stacked=True, colormap=cmap1, width=0.8, edgecolor='none', ax=axs[i, 0]\n",
    "    )\n",
    "\n",
    "    # Add axis labels and a title to the plot\n",
    "    ax.set_xlabel('Sample')\n",
    "    ax.set_ylabel('Percentage of Mbp')\n",
    "    ax.set_title(f'Percentage of kept Mbp (Samples {start_idx + 1} to {end_idx})', pad=20)\n",
    "\n",
    "    # Set the limits of the y-axis\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "    # Add a legend to the plot\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(reversed(handles), reversed(labels), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    print(f\"{100*n/(num_subplots):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "    n=n+1\n",
    "\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as PNG and SVG files\n",
    "fig.savefig(output_percentage_kept_Mbp_png, format=\"png\", bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(output_percentage_kept_Mbp_svg, format=\"svg\", bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee1d011e",
   "metadata": {},
   "source": [
    "## Removed reads per QC step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b638e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists and variables\n",
    "sample_l=[]\n",
    "read_types=[]\n",
    "raw=[]\n",
    "trimmed=[]\n",
    "duk=[]\n",
    "euk=[]\n",
    "norm=[]\n",
    "reads=[]\n",
    "\n",
    "# Initialize n\n",
    "n=1\n",
    "\n",
    "# Loop through each sample in read_stats_df and populate the lists with relevant information\n",
    "for sample in (read_stats_df.index.to_list()):  \n",
    "    # Add sample name and read type to lists\n",
    "    sample_l.extend([sample]*3)\n",
    "    read_types.extend([\"R1\", \"R2\", \"U\"])\n",
    "    # Read in raw read counts for each read type and add to raw list\n",
    "    raw.append(int(open(raw_dir + \"/\" + sample + forward_tag + \"_read_count.txt\" , 'r').readline().strip()))\n",
    "    raw.append(int(open(raw_dir + \"/\" +  sample + reverse_tag + \"_read_count.txt\" , 'r').readline().strip()))\n",
    "    raw.append(int(0))\n",
    "    # Read in trimmed read counts for each read type and add to trimmed list\n",
    "    trimmed.append(int(open(clean_dir + \"/\" + sample + \"_forward_paired_read_count.txt\" , 'r').readline().strip()))\n",
    "    trimmed.append(int(open(clean_dir + \"/\" + sample + \"_reverse_paired_read_count.txt\" , 'r').readline().strip()))\n",
    "    trimmed.append(int(open(clean_dir + \"/\" + sample + \"_merged_unpaired.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    # Read in non-eukaryotic read counts for each read type and add to euk list\n",
    "    euk.append(int(open(clean_dir + \"/\" + sample + \"_forward_paired_noEuk.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    euk.append(int(open(clean_dir + \"/\" + sample + \"_reverse_paired_noEuk.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    euk.append(int(open(clean_dir + \"/\" + sample + \"_unpaired_noEuk.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    # Read in BBDuk-cleaned read counts for each read type and add to duk list\n",
    "    duk.append(int(open(clean_dir + \"/\" + sample + \"_forward_paired_clean.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    duk.append(int(open(clean_dir + \"/\" + sample + \"_reverse_paired_clean.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    duk.append(int(open(clean_dir + \"/\" + sample + \"_unpaired_clean.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    # Read in normalized read counts for each read type and add to norm list\n",
    "    norm.append(int(open(clean_dir + \"/\" + sample + \"_forward_paired_norm.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    norm.append(int(open(clean_dir + \"/\" + sample + \"_reverse_paired_norm.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "    norm.append(int(open(clean_dir + \"/\" + sample + \"_unpaired_norm.tot_read_count.txt\" , 'r').readline().strip()))\n",
    "\n",
    "\n",
    "    print(f\"{100*n/len(SAMPLES):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "    n=n+1\n",
    "\n",
    "# Create an empty pandas DataFrame\n",
    "df_counts = pd.DataFrame()\n",
    "\n",
    "# Add columns to the DataFrame\n",
    "df_counts[\"sample\"] = sample_l\n",
    "df_counts[\"type\"] = read_types\n",
    "df_counts[\"raw\"] = raw\n",
    "df_counts[\"trimmomatic\"] = trimmed\n",
    "df_counts[\"kraken\"] = euk\n",
    "df_counts[\"bbduk\"] = duk\n",
    "df_counts[\"norm\"] = norm\n",
    "\n",
    "# Calculate values for new columns and add them to the DataFrame\n",
    "df_counts[\"raw_s\"] = df_counts[\"raw\"] / 1000000\n",
    "df_counts[\"low_QC_reads_s\"] = (df_counts[\"raw\"] - df_counts[\"trimmomatic\"]) / 1000000\n",
    "df_counts[\"eukaryotic_reads_s\"] = (df_counts[\"trimmomatic\"] - df_counts[\"kraken\"]) / 1000000\n",
    "df_counts[\"bbduk_phix174_reads_s\"] = (df_counts[\"kraken\"] - df_counts[\"bbduk\"]) / 1000000\n",
    "df_counts[\"duplicate_reads_s\"] = (df_counts[\"bbduk\"] - df_counts[\"norm\"]) / 1000000\n",
    "df_counts[\"assembly_reads_s\"] = df_counts[\"norm\"] / 1000000\n",
    "\n",
    "df_counts[\"low_QC_reads_p\"] = df_counts[\"low_QC_reads_s\"] / df_counts[\"raw_s\"]\n",
    "df_counts[\"eukaryotic_reads_p\"] = df_counts[\"eukaryotic_reads_s\"] / df_counts[\"raw_s\"]\n",
    "df_counts[\"bbduk_phix174_reads_p\"] = df_counts[\"bbduk_phix174_reads_s\"] / df_counts[\"raw_s\"]\n",
    "df_counts[\"duplicate_reads_p\"] = df_counts[\"duplicate_reads_s\"] / df_counts[\"raw_s\"]\n",
    "df_counts[\"assembly_reads_p\"] = df_counts[\"assembly_reads_s\"] / df_counts[\"raw_s\"]\n",
    "\n",
    "# Create a new column by concatenating two existing columns\n",
    "df_counts[\"sample_long\"] = df_counts[\"sample\"] + \"_\" + df_counts[\"type\"]\n",
    "\n",
    "# Convert the DataFrame to an HTML table and write it to a file\n",
    "df_counts.to_html(output_step_qc_reads_html)\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_counts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70cee76c",
   "metadata": {},
   "source": [
    "### Number of kept/removed reads per QC step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize n\n",
    "n=1\n",
    "\n",
    "# Set figure size and font scale for seaborn\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# Set figure width based on number of samples\n",
    "fig_width = len(SAMPLES) * 0.4\n",
    "\n",
    "\n",
    "# Define legend text and create colormap\n",
    "legend_text = [\"Used for assembly\", \"Duplicate (removed)\", \"phiX174 (user contaminants)\", \"Eukaryotic contamination\", \"Low quality\"]\n",
    "\n",
    "# Filter the dataframe to only include R1 reads\n",
    "df_counts_paired = df_counts[df_counts[\"type\"] == \"R1\"]\n",
    "\n",
    "# Create subplots with fixed width for each subplot\n",
    "fig, axs = plt.subplots(num_subplots, 1, figsize=(samples_per_subplot * 0.4, 12 * num_subplots), squeeze=False)\n",
    "\n",
    "# Plot each subset of samples in separate subplots\n",
    "for i in range(num_subplots):\n",
    "    # print(i)\n",
    "    start_idx = i * samples_per_subplot\n",
    "    end_idx = min((i + 1) * samples_per_subplot, len(df_counts_paired))\n",
    "    subset_df = df_counts_paired.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Plot the data\n",
    "    ax = subset_df.plot(x=\"sample\", y=[\"assembly_reads_s\", \"duplicate_reads_s\", \"bbduk_phix174_reads_s\", \"eukaryotic_reads_s\", \"low_QC_reads_s\"],\n",
    "                        kind=\"bar\", stacked=True, colormap=cmap1, ax=axs[i, 0], width=0.8, edgecolor='none')\n",
    "\n",
    "    # Add legend, axis labels and title to each subplot\n",
    "    ax.set_xlabel('Sample')\n",
    "    ax.set_ylabel('Million reads')\n",
    "    ax.set_title(f'Sample Read Counts (Samples {start_idx + 1} to {end_idx})', pad=20)\n",
    "\n",
    "    # Add legend to the subplot\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(reversed(handles), reversed(legend_text), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    print(f\"{100*n/(num_subplots):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "    n=n+1\n",
    "    \n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as PNG and SVG files\n",
    "plt.savefig(output_steps_qc_reads_png, format=\"png\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.savefig(output_steps_qc_reads_svg, format=\"svg\", bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "973aff4a",
   "metadata": {},
   "source": [
    "### Percentage of kept/removed reads per QC step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize n\n",
    "n=1\n",
    "\n",
    "# Set figure size and font scale for seaborn\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# Set figure width based on number of samples\n",
    "fig_width = len(SAMPLES) * 0.4\n",
    "\n",
    "# Create subplots with fixed width for each subplot\n",
    "fig, axs = plt.subplots(num_subplots, 1, figsize=(samples_per_subplot * 0.4, 12 * num_subplots), squeeze=False)\n",
    "\n",
    "# Plot each subset of samples in separate subplots\n",
    "for i in range(num_subplots):\n",
    "    start_idx = i * samples_per_subplot\n",
    "    end_idx = min((i + 1) * samples_per_subplot, len(df_counts_paired))\n",
    "    subset_df = df_counts_paired.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Plot the data\n",
    "    ax = subset_df.plot(\n",
    "        x=\"sample\",\n",
    "        y=[\"assembly_reads_p\", \"duplicate_reads_p\", \"bbduk_phix174_reads_p\", \"eukaryotic_reads_p\", \"low_QC_reads_p\"],\n",
    "        kind=\"bar\", stacked=True, colormap=cmap1, ax=axs[i, 0], width=0.8, edgecolor='none'\n",
    "    )\n",
    "\n",
    "    # Add legend, axis labels and title to each subplot\n",
    "    ax.set_xlabel('Sample')\n",
    "    ax.set_ylabel('Proportion of raw reads')\n",
    "    ax.set_title(f'Sample Proportion of Raw Reads (Samples {start_idx + 1} to {end_idx})', pad=20)\n",
    "\n",
    "    # Add legend to the subplot\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(reversed(handles), reversed(legend_text), loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    print(f\"{100*n/(num_subplots):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "    n=n+1\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as PNG and SVG files\n",
    "plt.savefig(output_steps_qc_percentage_png, format=\"png\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.savefig(output_steps_qc_percentage_svg, format=\"svg\", bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_paired.to_csv(output_df_counts_paired)\n",
    "df_counts_paired"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "857e48b9",
   "metadata": {},
   "source": [
    "## Super Deduper PCR duplication statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf59dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size and font scale for seaborn\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Initialize variables\n",
    "maxim = 0\n",
    "percentages = []\n",
    "\n",
    "# Create a single plot\n",
    "fig, ax = plt.subplots()\n",
    "n=1\n",
    "# Loop through each sample\n",
    "for sample in SAMPLES:\n",
    "    # Load the JSON data from the file\n",
    "    with open(qc_dir + \"/\" + sample + \"_stats_pcr_duplicates.log\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract the relevant data and calculate percentage of duplicates\n",
    "    df = pd.DataFrame(data[0][\"Fragment\"][\"duplicate_saturation\"], columns=[\"reads\", \"dup\"]) / 1000000\n",
    "    percentages.append(df.iloc[-1][\"dup\"] * 100 / df.iloc[-1][\"reads\"])\n",
    "    \n",
    "    # Find the maximum value of \"reads\"\n",
    "    max_temp = df[\"reads\"].max()\n",
    "    if max_temp > maxim:\n",
    "        maxim = max_temp\n",
    "\n",
    "    n=n+1\n",
    "    print(f\"{100*n/len(SAMPLES):.2f}\" + \" %\", end='\\r', flush=True)\n",
    "\n",
    "    # Plot the data as a line plot using seaborn\n",
    "    sns.lineplot(x=\"reads\", y=\"dup\", data=df, err_style='band', color=colors_rarefaction[len(percentages) - 1], \n",
    "                 label=sample, linewidth=1, alpha=0.8, ax=ax)\n",
    "\n",
    "# Set the x and y limits of the plot\n",
    "ax.set(ylim=(0, maxim))\n",
    "ax.set(xlim=(0, maxim))\n",
    "\n",
    "# Set the x and y labels of the plot\n",
    "ax.set_xlabel(\"Million reads\")\n",
    "ax.set_ylabel(\"PCR duplicates (Million reads)\")\n",
    "\n",
    "# Add a legend to the plot\n",
    "if len(SAMPLES) <= SAMPLE_PLOT_LIM_LEGEND:\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=int(np.ceil(len(SAMPLES) / 25)))\n",
    "else:\n",
    "    ax.get_legend().set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as png and svg files\n",
    "fig.savefig(output_supperdedupper_png, format=\"png\", bbox_inches=\"tight\", transparent=True)\n",
    "fig.savefig(output_supperdedupper_svg, format=\"svg\", bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "# Create a dataframe to store the percentage of duplicates for each sample\n",
    "pcr_dup_df = pd.DataFrame({\n",
    "    \"sample\": SAMPLES,\n",
    "    \"pcr_percent_duplicates\": percentages\n",
    "})\n",
    "\n",
    "# Generate a styled HTML table of the dataframe and save it to a file\n",
    "pcr_dup_df_out = pcr_dup_df.style.background_gradient(cmap=\"RdYlGn_r\", vmin=0, vmax=100).render()\n",
    "with open(output_supperdedupper_html, \"w\") as fp:\n",
    "    fp.write(pcr_dup_df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff67625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the styled dataframe in the notebook\n",
    "pcr_dup_df.style.background_gradient(cmap=\"RdYlGn_r\", vmin=0, vmax=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
