{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import colors\n",
    "\n",
    "#------------------------------------------\n",
    "cmap1=LinearSegmentedColormap.from_list(\"my_colormap\", sns.color_palette(\"colorblind\", n_colors=5))\n",
    "\n",
    "\n",
    "sns.set_style(\"ticks\",{'axes.grid' : True})\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.5\n",
    "plt.rcParams[\"xtick.major.width\"] = 1.5\n",
    "plt.rcParams[\"ytick.major.width\"] = 1.5\n",
    "plt.rcParams[\"xtick.major.size\"] = 8\n",
    "plt.rcParams[\"ytick.major.size\"] = 8\n",
    "plt.rcParams[\"axes.titlepad\"] = 20\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams[\"axes.titlesize\"] = 30\n",
    "plt.rcParams['axes.labelsize'] = 23.5\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Liberation Sans']\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams[\"savefig.dpi\"]=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b75824",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING=snakemake.params.sampling\n",
    "illumina_postqc=snakemake.input.postqc_txt\n",
    "mapping_dir=snakemake.params.mapping_dir\n",
    "clean_dir=snakemake.params.clean_dir\n",
    "threshold_bases=snakemake.params.threshold_bases\n",
    "REFERENCE=snakemake.params.reference\n",
    "THRESHOLD_RPKM=snakemake.params.threshold_RPKM\n",
    "\n",
    "out_raw_RPKM_file=snakemake.output.raw_RPKM_file\n",
    "out_norm_RPKM_file=snakemake.output.norm_RPKM_file\n",
    "out_raw_count_file=snakemake.output.raw_count_file\n",
    "out_norm_count_file=snakemake.output.norm_count_file\n",
    "out_coverage_RPKM_file=snakemake.output.coverage_RPKM_file\n",
    "out_coverage_bases_RPKM_file=snakemake.output.coverage_bases_RPKM_file\n",
    "\n",
    "out_filtered_raw_RPKM_file=snakemake.output.filtered_raw_RPKM_file\n",
    "out_filtered_norm_RPKM_file=snakemake.output.filtered_norm_RPKM_file\n",
    "out_filtered_raw_count_file=snakemake.output.filtered_raw_count_file\n",
    "out_filtered_norm_count_file=snakemake.output.filtered_norm_count_file\n",
    "out_filtered_75_raw_RPKM_file=snakemake.output.filtered_75_raw_RPKM_file\n",
    "out_filtered_75_norm_RPKM_file=snakemake.output.filtered_75_norm_RPKM_file\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6227c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "postqc = pd.read_csv(illumina_postqc, sep=\"\\t\")\n",
    "\n",
    "# # create a new dataframe with columns for Filename, average sequence length, and total sequences from the postqc dataframe\n",
    "read_stats_df2 = postqc[[\"Filename\",\"avg_sequence_length\"]]\n",
    "\n",
    "# replace certain portions of the Filename column with standard suffixes (e.g. _forward -> _R1)\n",
    "read_stats_df2.loc[:, \"Filename\"] = read_stats_df2[\"Filename\"].str.replace(\"_forward\", \"_R1\")\n",
    "read_stats_df2.loc[:, \"Filename\"] = read_stats_df2[\"Filename\"].str.replace(\"_reverse\", \"_R2\")\n",
    "read_stats_df2.loc[:, \"Filename\"] = read_stats_df2[\"Filename\"].str.replace(\"_unpaired\", \"_U\")\n",
    "read_stats_df2.loc[:, \"Filename\"] = read_stats_df2[\"Filename\"].str.split(\"_paired\").str[0]\n",
    "read_stats_df2.loc[:, \"Filename\"] = read_stats_df2[\"Filename\"].str.split(\"_clean\").str[0]\n",
    "\n",
    "# create a new columns in read_stats_df2 for the sample name, based on the first part of the Filename (before the final underscore)\n",
    "read_stats_df2.loc[:, \"Sample\"] = read_stats_df2[\"Filename\"].str.rsplit(\"_\",n=1).str[0]\n",
    "read_stats_df2.loc[:, \"Read_type\"] = read_stats_df2[\"Filename\"].str.rsplit(\"_\",n=1).str[1]\n",
    "\n",
    "# set the Filename column as the index of read_stats_df2\n",
    "read_stats_df2 = read_stats_df2.set_index('Filename')\n",
    "\n",
    "# rename the columns of read_stats_df2\n",
    "read_stats_df2.columns=[\"length_post\", \"sample\", \"Read_type\"]\n",
    "read_stats_df2=read_stats_df2[read_stats_df2[\"Read_type\"]!=\"U\"]\n",
    "read_length_paired=read_stats_df2.groupby(\"sample\").mean(numeric_only=True)\n",
    "read_length_paired.loc[:, \"length_post\"] = read_length_paired[\"length_post\"].round(2)\n",
    "read_length_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d013b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES=read_length_paired.index.to_list()\n",
    "SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Written by Alejandro Reyes\n",
    "# Reformatted & Edited Laura Forero\n",
    "\n",
    "def normalise_rpkm_bt2(sample,all_file, toss_file, paired_len):\n",
    "    all_df=pd.read_csv(all_file, sep=\"\\t\")\n",
    "    toss_df=pd.read_csv(toss_file, sep=\"\\t\")\n",
    "\n",
    "    all_df.columns=[\"ID\", \"Avg_fold\", \"Length\", \"Covered_bases\", \"Read_Count\", \"Variance\", \"Trimmed Mean\", \"RPKM_\" + sample]\n",
    "    all_df[\"Covered_percent\"]=all_df[\"Covered_bases\"]*100/all_df[\"Length\"]\n",
    "    all_df=all_df.drop(columns=[\"Avg_fold\",\"Variance\", \"Trimmed Mean\"])\n",
    "    all_df=all_df.add_suffix('_all')\n",
    "\n",
    "    toss_df.columns=[\"ID\", \"Avg_fold\", \"Length\", \"Covered_bases\", \"Read_Count\", \"Variance\", \"Trimmed Mean\", \"RPKM_\" + sample]\n",
    "    toss_df[\"Covered_percent\"]=toss_df[\"Covered_bases\"]*100/toss_df[\"Length\"]\n",
    "    toss_df=toss_df.drop(columns=[ \"Avg_fold\",\"Variance\", \"Trimmed Mean\"])\n",
    "    toss_df=toss_df.add_suffix('_toss')\n",
    "\n",
    "    \n",
    "    combined_df=all_df.merge(toss_df, left_on=\"ID_all\", right_on=\"ID_toss\")\n",
    "    combined_df=combined_df.drop(columns=[\"ID_toss\", \"Length_toss\"])\n",
    "    \n",
    "    combined_df[\"extra_reads\"]=abs(combined_df[\"Read_Count_all\"]-combined_df[\"Read_Count_toss\"])\n",
    "    combined_df[\"test\"]= (combined_df[\"Read_Count_all\"]* 1E6/((combined_df[\"Length_all\"]/1000)*combined_df[\"Read_Count_all\"].sum()))\n",
    "\n",
    "    combined_df[\"toss_cov\"] = combined_df.apply(lambda row: (row[\"Read_Count_toss\"] / row[\"Covered_bases_toss\"]) if row[\"Covered_bases_toss\"] > 0 else 0, axis=1)\n",
    "    combined_df[\"delta_map_base\"] = abs(combined_df[\"Covered_bases_all\"]-combined_df[\"Covered_bases_toss\"])\n",
    "    combined_df[\"needed_reads\"] = round(combined_df[\"toss_cov\"]*combined_df[\"delta_map_base\"]*0.9)\n",
    "\n",
    "    combined_df[\"extra\"] = combined_df.apply(lambda row: min(abs(row[\"extra_reads\"]), abs(row[\"needed_reads\"])), axis=1)\n",
    "    \n",
    "    combined_df[\"cov_ratio\"]=np.log10((combined_df[\"Covered_percent_all\"]+1)/(combined_df[\"Covered_percent_toss\"]+1))\n",
    "\n",
    "    combined_df[\"needed_ratio\"]=np.log10((combined_df[\"needed_reads\"]+1)/(combined_df[\"extra_reads\"]+1))\n",
    "    combined_df[\"Read_count_norm_\" + sample] = (combined_df[\"Read_Count_toss\"]+combined_df[\"extra\"]).astype('int')\n",
    "\n",
    "    threshold = threshold_bases \n",
    "    combined_df.loc[(combined_df[\"Covered_bases_toss\"] < threshold) & (combined_df[\"Read_count_norm_\" + sample] > 0), [\"Read_count_norm_\" + sample]] = 0\n",
    "\n",
    "    total_reads_map = combined_df[\"Read_Count_toss\"].sum()\n",
    "    total_reads_map=total_reads_map+combined_df[\"extra\"].sum()\n",
    "    combined_df[\"RPKM_norm_\" + sample] = ((combined_df[\"Read_count_norm_\" + sample])* 1E9)/(combined_df[\"Length_all\"]*total_reads_map)\n",
    "    \n",
    "    combined_df[\"exp_ratio\"] = combined_df.apply(lambda row: np.log10(100 * (1 - math.exp(-1 * ((row[\"Read_Count_toss\"] + row[\"needed_reads\"]) * paired_len) / row[\"Length_all\"])) / row[\"Covered_percent_all\"]) if (row[\"Read_Count_toss\"] > 0) and (total_reads_map > 0) else 0, axis=1)\n",
    "    \n",
    "    combined_df=combined_df[['ID_all', 'Length_all', \"Read_Count_all\", \"Read_Count_toss\",'RPKM_'+ sample + '_all', 'Covered_percent_all' , 'RPKM_'+ sample + '_toss', 'Covered_percent_toss', \"Read_count_norm_\" + sample, 'RPKM_norm_'+ sample, \"Covered_bases_all\", \"Covered_bases_toss\"]]\n",
    "    combined_df.columns=['Contig', 'Length_' + sample,\"Count_\" + sample, \"Read_Count_toss_\" + sample,'RPKM_'+ sample + '_all', 'Covered_percent_all_'+ sample , 'RPKM_'+ sample + '_toss', 'Covered_percent_toss_' + sample, \"Count_norm_\" + sample , 'RPKM_norm_'+ sample, \"Covered_bases_all_\" + sample, \"Covered_bases_toss_\" + sample ]\n",
    "    return(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_abundances(RPKM, counts, coverage_bases, coverage_percent, contig_length):\n",
    "    contig_lengths_6667 = contig_length >= 6667\n",
    "    bases_covered_gt_5000 = coverage_bases > 5000\n",
    "    percentage_covered_gt_75 = coverage_percent > 75\n",
    "\n",
    "    # # Create a filtered abundance DataFrame with the same shape as RPKM_raw, initialized with zeros\n",
    "    filtered_RPKM_df = pd.DataFrame(0, index=RPKM.index, columns=RPKM.columns)\n",
    "    filtered_counts_df = pd.DataFrame(0, index=RPKM.index, columns=RPKM.columns)\n",
    "\n",
    "    condition1 = contig_lengths_6667 & bases_covered_gt_5000\n",
    "    condition2 = ~contig_lengths_6667 & percentage_covered_gt_75\n",
    "\n",
    "    filtered_RPKM_df[condition1] = RPKM[condition1]\n",
    "    filtered_RPKM_df[condition2] = RPKM[condition2]\n",
    "    \n",
    "    filtered_counts_df[condition1] = counts[condition1]\n",
    "    filtered_counts_df[condition2] = counts[condition2]\n",
    "    \n",
    "    # threshold_RPKM=THRESHOLD_RPKM\n",
    "    # filtered_RPKM_df[filtered_RPKM_df<threshold_RPKM]=0\n",
    "    # filtered_counts_df[filtered_RPKM_df<threshold_RPKM]=0\n",
    "    \n",
    "    return(filtered_RPKM_df, filtered_counts_df)\n",
    "\n",
    "def filter_abundances_75(RPKM, coverage_bases, coverage_percent, contig_length):\n",
    "    percentage_covered_gt_75 = coverage_percent > 75\n",
    "\n",
    "    # # Create a filtered abundance DataFrame with the same shape as RPKM_raw, initialized with zeros\n",
    "    filtered_RPKM_df = pd.DataFrame(0, index=RPKM.index, columns=RPKM.columns)\n",
    "\n",
    "    filtered_RPKM_df[percentage_covered_gt_75] = RPKM[percentage_covered_gt_75]\n",
    "\n",
    "    return(filtered_RPKM_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e29059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"NUMBER OF SAMPLES =\" , len(SAMPLES))\n",
    "\n",
    "n=0\n",
    "for sample in SAMPLES:\n",
    "    print(round(((n+1) * 100 / len(SAMPLES)),1), \"%\", end='\\r')\n",
    "    if len(REFERENCE)>0:\n",
    "        all_file=mapping_dir + \"/REFERENCES/bowtie2_\" + REFERENCE + \"_\" + sample + \"_\" + SAMPLING + \"_covstats.txt\"\n",
    "        toss_file=mapping_dir + \"/REFERENCES/bowtie2_\" + REFERENCE + \"_\" + sample + \"_\" + SAMPLING + \"_unique_covstats.txt\"\n",
    "    else:\n",
    "        all_file=mapping_dir + \"/bowtie2_\" + sample + \"_\" + SAMPLING + \"_covstats.txt\"\n",
    "        toss_file=mapping_dir + \"/bowtie2_\" + sample + \"_\"+ SAMPLING + \"_unique_covstats.txt\"\n",
    "    \n",
    "    read_len=int(read_length_paired.loc[sample][\"length_post\"])\n",
    "    count_df=normalise_rpkm_bt2(sample,all_file, toss_file, read_len)  \n",
    "\n",
    "    if n == 0:\n",
    "        df_RPKM_norm = count_df[[\"Contig\", \"RPKM_norm_\" + sample]]\n",
    "        df_RPKM_raw = count_df[[\"Contig\", \"RPKM_\" + sample + \"_all\"]]\n",
    "        df_count_raw = count_df[[\"Contig\", \"Count_\" + sample ]]\n",
    "        df_count_norm = count_df[[\"Contig\", \"Count_norm_\" + sample ]]\n",
    "        df_RPKM_cov = count_df[[\"Contig\", \"Covered_percent_all_\"+ sample]]\n",
    "        df_RPKM_cov_bases = count_df[[\"Contig\", \"Covered_bases_all_\"+ sample]]\n",
    "        df_RPKM_len = count_df[[\"Contig\", \"Length_\" + sample]]\n",
    "        \n",
    "    else:\n",
    "        df_RPKM_norm = df_RPKM_norm.merge(count_df[[\"Contig\", \"RPKM_norm_\" + sample]], on=\"Contig\")\n",
    "        df_RPKM_raw = df_RPKM_raw.merge(count_df[[\"Contig\", \"RPKM_\" + sample + \"_all\"]], on=\"Contig\")\n",
    "        df_count_raw = df_count_raw.merge(count_df[[\"Contig\", \"Count_\" + sample]], on=\"Contig\")\n",
    "        df_count_norm = df_count_norm.merge(count_df[[\"Contig\", \"Count_norm_\" + sample ]], on=\"Contig\")\n",
    "        df_RPKM_cov = df_RPKM_cov.merge(count_df[[\"Contig\", \"Covered_percent_all_\"+ sample]], on=\"Contig\")\n",
    "        df_RPKM_cov_bases = df_RPKM_cov_bases.merge(count_df[[\"Contig\", \"Covered_bases_all_\"+ sample]], on=\"Contig\")\n",
    "        df_RPKM_len = df_RPKM_len.merge(count_df[[\"Contig\", \"Length_\" + sample]], on=\"Contig\")\n",
    "        \n",
    "    n=n+1\n",
    "\n",
    "\n",
    "df_RPKM_raw=df_RPKM_raw.rename(columns={\"Contig\": \"vOTU\"}).set_index('vOTU')\n",
    "df_RPKM_norm=df_RPKM_norm.rename(columns={\"Contig\": \"vOTU\"}).set_index('vOTU')\n",
    "df_count_raw=df_count_raw.rename(columns={\"Contig\": \"vOTU\"}).set_index('vOTU')\n",
    "df_count_norm=df_count_norm.rename(columns={\"Contig\": \"vOTU\"}).set_index('vOTU')\n",
    "df_RPKM_cov=df_RPKM_cov.rename(columns={\"Contig\": \"vOTU\"}).set_index('vOTU')\n",
    "df_RPKM_cov_bases=df_RPKM_cov_bases.rename(columns={\"Contig\": \"vOTU\"}).set_index('vOTU')\n",
    "df_RPKM_len=df_RPKM_len.rename(columns={\"Contig\": \"vOTU\"}).set_index('vOTU')\n",
    "\n",
    "\n",
    "df_RPKM_raw.columns = df_RPKM_raw.columns.str.strip('RPKM_').str.rstrip('_all')\n",
    "df_RPKM_norm.columns = df_RPKM_norm.columns.str.strip('RPKM_norm_').str.rstrip('_all')\n",
    "df_count_raw.columns = df_count_raw.columns.str.replace('Count_', \"\")\n",
    "df_count_norm.columns = df_count_norm.columns.str.replace('Count_norm_', \"\")\n",
    "df_RPKM_cov.columns = df_RPKM_cov.columns.str.replace('Covered_percent_all_', \"\")\n",
    "df_RPKM_cov_bases.columns = df_RPKM_cov_bases.columns.str.replace('Covered_bases_all_', \"\")\n",
    "df_RPKM_len.columns = df_RPKM_len.columns.str.replace('Length_', \"\")\n",
    "\n",
    "df_mean_coverage_raw=df_count_raw.div(df_RPKM_len)\n",
    "df_mean_coverage_norm=df_count_norm.div(df_RPKM_len)\n",
    "\n",
    "df_RPKM_raw.to_csv(out_raw_RPKM_file)\n",
    "df_RPKM_norm.to_csv(out_norm_RPKM_file)\n",
    "df_RPKM_cov.to_csv(out_coverage_RPKM_file)\n",
    "df_RPKM_cov_bases.to_csv(out_coverage_bases_RPKM_file)\n",
    "df_count_raw.to_csv(out_raw_count_file)\n",
    "df_count_norm.to_csv(out_norm_count_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38cbf6",
   "metadata": {},
   "source": [
    "## Subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bde895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped_sub=pd.DataFrame()\n",
    "\n",
    "if SAMPLING==\"sub\":\n",
    "    mapped_pair=[]\n",
    "    sub_reads=[]\n",
    "    for sample in SAMPLES:\n",
    "        content = open(mapping_dir + \"/bowtie2_flagstats_filtered_\" + sample + \".\" +  \"sub.txt\").readlines()\n",
    "        mapped_pair.append(int(content[1].split()[0]) / 2)\n",
    "        content2 = open(clean_dir + \"/\" + sample + \"_sub_sampling_reads_final.txt\").readline()\n",
    "        sub_reads.append(int(content2))\n",
    "\n",
    "    df_mapped_sub[\"sample\"]=SAMPLES\n",
    "    df_mapped_sub[\"mapped\"]=mapped_pair\n",
    "    df_mapped_sub[\"sub_reads\"]=sub_reads\n",
    "    df_mapped_sub[\"viral_reads\"]=df_mapped_sub[\"mapped\"]\n",
    "\n",
    "    df_mapped_sub[\"%mapped\"]=df_mapped_sub[\"mapped\"]*100/df_mapped_sub[\"sub_reads\"]\n",
    "    df_mapped_sub.loc['mean'] = df_mapped_sub.mean(numeric_only=True)\n",
    "    df_mapped_sub=df_mapped_sub.sort_values(by=\"viral_reads\")\n",
    "    # df_mapped_sub.sort_values(by=\"viral_reads\").style.set_precision(2).background_gradient(cmap=\"RdYlGn\", vmin=0)\n",
    "df_mapped_sub.style.background_gradient(cmap=\"RdYlGn\", vmin=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad2430",
   "metadata": {},
   "source": [
    "## RPKM normalised vs raw\n",
    "Percentage indicates the ammount of contigs that lost coverage (0 now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18651114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_RPKM_norm2=df_RPKM_norm.copy()\n",
    "df_RPKM_raw2=df_RPKM_raw.copy()\n",
    "df_RPKM_cov2=df_RPKM_cov.copy()\n",
    "\n",
    "col_names_norm = [col + \"_norm\" for col in df_RPKM_norm.columns]\n",
    "col_names_raw = [col + \"_raw\" for col in df_RPKM_raw.columns]\n",
    "col_names_cov = [col + \"_cov\" for col in df_RPKM_cov.columns]\n",
    "\n",
    "df_RPKM_norm2.columns = col_names_norm\n",
    "df_RPKM_raw2.columns = col_names_raw\n",
    "df_RPKM_cov2.columns = col_names_cov\n",
    "\n",
    "\n",
    "max_val=max(df_RPKM_norm.select_dtypes(include=[float]).max().max(),df_RPKM_raw.select_dtypes(include=[float]).max().max())\n",
    "min_val=min(df_RPKM_norm.select_dtypes(include=[float]).replace(0, np.nan).min().min(),df_RPKM_raw.select_dtypes(include=[float]).replace(0, np.nan).min().min())\n",
    "merged_df = df_RPKM_norm2.merge(df_RPKM_raw2, left_index=True,right_index=True).merge(df_RPKM_cov2, left_index=True,right_index=True)\n",
    "\n",
    "\n",
    "# Determine common axis limits for log scale\n",
    "max_val = max(df_RPKM_norm.select_dtypes(include=[float]).max().max(),\n",
    "              df_RPKM_raw.select_dtypes(include=[float]).max().max())\n",
    "min_val = min(df_RPKM_norm.select_dtypes(include=[float]).replace(0, np.nan).min().min(),\n",
    "              df_RPKM_raw.select_dtypes(include=[float]).replace(0, np.nan).min().min())\n",
    "\n",
    "# Set the limits to be the same for x and y axis\n",
    "axis_limits = [min_val, max_val]\n",
    "\n",
    "\n",
    "n_columns=4\n",
    "# Set up the subplots\n",
    "len_samples=len(SAMPLES)\n",
    "if len_samples == 1:\n",
    "    len_samples = 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=(len_samples + n_columns-1) // n_columns, ncols=n_columns, figsize=(n_columns*3.74, ((len_samples + n_columns -1) // n_columns)*4.5), sharey=True, sharex=False)\n",
    "\n",
    "# Your existing loop for plotting...\n",
    "for i, sample in enumerate(SAMPLES):\n",
    "    row = i // n_columns\n",
    "    col = i % n_columns\n",
    "    ax = axes[row, col]\n",
    "    print(round(((i+1) * 100 / len(SAMPLES)),1), \"%\", end='\\r')\n",
    "    percent_0=round((len(merged_df[merged_df[sample + \"_raw\"]==0])/len(merged_df)-len(merged_df[merged_df[sample + \"_norm\"]==0])/len(merged_df))*100,2)\n",
    "    ax.plot([-2.5, max_val-0.5], [-2.5, max_val-0.5], 'r--')\n",
    "    ax.scatter(x=merged_df[sample + \"_raw\"], y=merged_df[sample + \"_norm\"], s=2, alpha=0.3)\n",
    "\n",
    "    ax.set_xlim(axis_limits)\n",
    "    ax.set_ylim(axis_limits)\n",
    "    ax.set_title(sample)\n",
    "    ax.set_xlabel(\"RPKM raw\")\n",
    "    ax.set_ylabel(\"RPKM normalised\")\n",
    "\n",
    "    ax.text(0.95, 0.05, str(percent_0) + '%', transform=ax.transAxes, ha='right', va='bottom',fontsize=20, color=\"firebrick\")\n",
    "\n",
    "    \n",
    "    # Set the same ticks for both axes\n",
    "    ax.set_xticklabels(ax.get_xticks(), rotation=90)\n",
    "\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')   \n",
    "    \n",
    "    ax.set_xticks([10**i for i in range(int(np.log10(min_val)), int(np.log10(max_val))+1)])\n",
    "    ax.set_yticks([10**i for i in range(int(np.log10(min_val)), int(np.log10(max_val))+1)])\n",
    "#     ax.set_yticks([10**i for i in range(int(np.log10(min_val)), int(np.log10(max_val))+1)])\n",
    "\n",
    " \n",
    "    \n",
    "if len(SAMPLES) % n_columns != 0:\n",
    "    for i in range(len(SAMPLES) % n_columns, n_columns):\n",
    "        fig.delaxes(axes[-1, i])\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350a621",
   "metadata": {},
   "source": [
    "## Length and raw RPKM of removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df=df_RPKM_len[SAMPLES[0]].to_frame()\n",
    "len_df.columns=[\"Length\"]\n",
    "\n",
    "merged_df = merged_df.sort_index()\n",
    "len_df = len_df.sort_index()\n",
    "\n",
    "len_samples=len(SAMPLES)\n",
    "if len_samples == 1:\n",
    "    len_samples = 2\n",
    "fig, axes = plt.subplots(nrows=(len_samples+ 3) // 4, ncols=4, figsize=(16, ((len_samples + 3) // 4)*4.5),sharey=False,sharex=True)\n",
    "\n",
    "n=0\n",
    "\n",
    "for i, sample in enumerate(SAMPLES):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    ax = axes[row, col]\n",
    "    zero_df_len=len_df[(merged_df[sample + \"_raw\"]>0) & (merged_df[sample + \"_norm\"]==0)]\n",
    "    zero_df=merged_df[(merged_df[sample + \"_raw\"]>0) & (merged_df[sample + \"_norm\"]==0)]\n",
    "    print(round(((i+1) * 100 / len(SAMPLES)),1), \"%\", end='\\r')\n",
    "\n",
    "    percent_0=round((len(merged_df[merged_df[sample + \"_raw\"]==0])/len(merged_df)-len(merged_df[merged_df[sample + \"_norm\"]==0])/len(merged_df))*100,2)\n",
    "    bin_edges = [i * 1000 for i in range(0, 50)]\n",
    "    ax.scatter( x=zero_df_len[\"Length\"],y=zero_df[sample + \"_raw\"],)\n",
    "    ax.set_title(sample)\n",
    "    ax.set_xlabel(\"Contig length\")\n",
    "    ax.set_ylabel(\"RPKM\")\n",
    "    ax.text(0.95, 0.05, 'n=' + str(len(zero_df)) , transform=ax.transAxes, ha='right', va='bottom',fontsize=20, color=\"firebrick\")\n",
    "\n",
    "if len(SAMPLES) % 4 != 0:\n",
    "    for i in range(len(SAMPLES) % 4, 4):\n",
    "        fig.delaxes(axes[-1, i])\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter by Coverage + RPKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_RPKM_raw_df,filtered_counts_raw_df=filter_abundances(df_RPKM_raw, df_count_raw, df_RPKM_cov_bases, df_RPKM_cov, df_RPKM_len)\n",
    "filtered_RPKM_norm_df,filtered_counts_norm_df=filter_abundances(df_RPKM_norm, df_count_norm, df_RPKM_cov_bases, df_RPKM_cov, df_RPKM_len)\n",
    "filtered_75_RPKM_raw_df=filter_abundances_75(df_RPKM_raw, df_RPKM_cov_bases, df_RPKM_cov, df_RPKM_len)\n",
    "filtered_75_RPKM_norm_df=filter_abundances_75(df_RPKM_norm, df_RPKM_cov_bases, df_RPKM_cov, df_RPKM_len)\n",
    "\n",
    "filtered_RPKM_raw_df.to_csv(out_filtered_raw_RPKM_file)\n",
    "filtered_counts_raw_df.to_csv(out_filtered_raw_count_file)\n",
    "filtered_RPKM_norm_df.to_csv(out_filtered_norm_RPKM_file)\n",
    "filtered_counts_norm_df.to_csv(out_filtered_norm_count_file)\n",
    "filtered_75_RPKM_raw_df.to_csv(out_filtered_75_raw_RPKM_file)\n",
    "filtered_75_RPKM_norm_df.to_csv(out_filtered_75_norm_RPKM_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
